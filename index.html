<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <title>TensorFlow.js MarianMT Model Demo</title>
        <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.18.0/dist/tf.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@xenova/transformers@2.13.0/dist/transformers.min.js"></script>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 2em;
            }
            textarea {
                width: 100%;
            }
            #output {
                margin-top: 1em;
                font-size: 1.2em;
                color: #333;
            }
        </style>
    </head>
    <body>
        <h1>TensorFlow.js MarianMT Model Demo</h1>
        <textarea
            id="input"
            rows="4"
            cols="50"
            placeholder="Enter English text"
        ></textarea
        ><br />
        <button id="translate-btn">Translate</button>
        <h2>Ukrainian Translation:</h2>
        <div id="output"></div>

        <script>
            // Adjust this path if your model files are in a different location
            const MODEL_URL = "tfjs_model/model.json";

            let model,
                tokenizer,
                ready = false;

            async function loadModelAndTokenizer() {
                document.getElementById("output").innerText =
                    "Loading model and tokenizer...";
                try {
                    model = await tf.loadGraphModel(MODEL_URL);
                    tokenizer =
                        await window.transformers.AutoTokenizer.from_pretrained(
                            "Helsinki-NLP/opus-mt-en-uk"
                        );
                    ready = true;
                    document.getElementById("output").innerText = "Ready!";
                } catch (e) {
                    document.getElementById("output").innerText =
                        "Failed to load model or tokenizer.";
                    console.error(e);
                }
            }

            async function translate() {
                if (!ready) {
                    document.getElementById("output").innerText =
                        "Model not loaded yet.";
                    return;
                }
                const inputText = document.getElementById("input").value.trim();
                if (!inputText) {
                    document.getElementById("output").innerText =
                        "Please enter text.";
                    return;
                }
                document.getElementById("output").innerText = "Translating...";

                // Tokenize input
                const encoded = await tokenizer.encode(inputText);
                const input_ids = tf.tensor(
                    [encoded.ids],
                    [1, encoded.ids.length],
                    "int32"
                );
                const attention_mask = tf.tensor(
                    [encoded.attentionMask],
                    [1, encoded.attentionMask.length],
                    "int32"
                );

                // Prepare decoder input (start with <pad> token)
                const pad_id = tokenizer.token_to_id("<pad>");
                const eos_id = tokenizer.token_to_id("</s>");
                let output_ids = [];
                let next_input = tf.tensor([[pad_id]], [1, 1], "int32");
                let next_mask = tf.onesLike(next_input);

                // Greedy decoding loop
                for (let i = 0; i < 50; ++i) {
                    const output = model.execute({
                        input_ids,
                        attention_mask,
                        decoder_input_ids: next_input,
                        decoder_attention_mask: next_mask,
                    });

                    // Get logits for the last token
                    const logits = output.logits ? output.logits : output[0];
                    const last_token_logits = logits.slice(
                        [0, -1, 0],
                        [1, 1, -1]
                    );
                    const last_token_id = (
                        await last_token_logits.argMax(-1).array()
                    )[0][0];

                    if (last_token_id === eos_id) break;
                    output_ids.push(last_token_id);

                    // Prepare next decoder input
                    const ids = [pad_id, ...output_ids];
                    next_input = tf.tensor([ids], [1, ids.length], "int32");
                    next_mask = tf.onesLike(next_input);
                }

                // Decode output tokens
                const translation = await tokenizer.decode(output_ids, {
                    skip_special_tokens: true,
                });
                document.getElementById("output").innerText =
                    translation || "(No translation produced)";
            }

            document.getElementById("translate-btn").onclick = translate;
            loadModelAndTokenizer();
        </script>
    </body>
</html>
